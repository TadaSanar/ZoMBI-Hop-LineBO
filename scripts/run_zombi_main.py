"""
Database-driven ZoMBI-Hop runner (LineBO + serial/DB handshake).

This module contains the DB-backed objective + runner that used to live in
`zombihop_linebo_v2.py`.
"""

from __future__ import annotations

import csv
import json
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple


import numpy as np
import torch

from src import ZoMBIHop, LineBO
from src.core.linebo import batch_line_simplex_segments, line_simplex_segment, zero_sum_dirs

from scripts import communication

try:
    import matplotlib
    matplotlib.use("TkAgg")
    import matplotlib.pyplot as plt
    _HAS_MPL = True
except Exception:
    _HAS_MPL = False

# # Default Configuration
# DEFAULT_DIMENSIONS = 10
# DEFAULT_NUM_MINIMA = 3
# DEFAULT_TIME_LIMIT_HOURS = 24.0
NUM_EXPERIMENTS = 24
NUM_INIT_DATA = 2
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Database interfacing settings
OPTIMIZING_DIMS = [0, 8, 9]


# --- Live plotting and iteration logging ---
def setup_live_plots() -> Tuple[Dict[str, Any], List[float], List[float], List[np.ndarray], Dict[str, Any]]:
    """Return (fig_ref, all_sample_num, all_y, all_x_actual, last_call_ref). No window yet; plots are created on first update and then closed/recreated each time."""
    if not _HAS_MPL:
        return {}, [], [], [], {}
    fig_ref: Dict[str, Any] = {}  # will hold 'fig' so we can close it before recreating
    return fig_ref, [], [], [], {}


def update_live_plots(
    fig_ref: Dict[str, Any],
    all_sample_num: List[float],
    all_y: List[float],
    all_x_actual: List[np.ndarray],
    new_x_actual: np.ndarray,
    new_y: np.ndarray,
    needle_plot_points: List[Dict[str, float]] | None = None,
) -> None:
    """Append new points, close previous plot window, create a new figure with full state, show it (non-blocking). Plots are not kept active."""
    if not _HAS_MPL:
        return
    new_x = np.atleast_2d(new_x_actual)
    new_y_flat = np.atleast_1d(new_y).ravel()
    n_new = len(new_y_flat)
    for i in range(n_new):
        all_sample_num.append(len(all_sample_num) + 1)
        all_y.append(float(new_y_flat[i]))
        all_x_actual.append(new_x[i] if new_x.shape[0] > i else new_x[0])
    if not all_x_actual:
        return
    # Close previous figure so we don't keep active windows
    prev_fig = fig_ref.get("fig")
    if prev_fig is not None:
        try:
            plt.close(prev_fig)
        except Exception:
            pass
        fig_ref["fig"] = None
    X = np.array(all_x_actual)
    center = np.mean(X, axis=0)
    # center = np.ones((len(OPTIMIZING_DIMS),)) * 1.0 / len(OPTIMIZING_DIMS)
    distances = np.linalg.norm(X - center, axis=1)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    fig.suptitle("ZoMBI-Hop live")
    ax1.set_xlabel("Sample number")
    ax1.set_ylabel("Objective value")
    ax1.set_title("Objective value vs sample number")
    ax1.plot(all_sample_num, all_y, "b.-", markersize=4)
    ax2.set_xlabel("Distance from center of mass")
    ax2.set_ylabel("Objective value")
    ax2.set_title("Objective value vs distance from center")
    n_pts = len(all_y)
    iteration_order = np.arange(n_pts)
    scatter_ax2 = ax2.scatter(
        distances, all_y, c=iteration_order, ec="k", lw=0.3, cmap="viridis", s=8, alpha=1, vmin=0, vmax=max(n_pts - 1, 1)
    )
    fig.colorbar(scatter_ax2, ax=ax2, label="Iteration (dark=oldest, yellow=newest)")
    if needle_plot_points:
        for n in needle_plot_points:
            ax1.scatter(n["sample_idx"], n["y"], marker="*", s=200, c="gold", zorder=5, edgecolors="darkgoldenrod")
            ax2.scatter(n["distance"], n["y"], marker="*", s=200, c="gold", zorder=5, edgecolors="darkgoldenrod")
    fig_ref["fig"] = fig
    plt.show(block=False)
    plt.pause(0.001)  # allow GUI to update


def log_iteration(
    candidate: torch.Tensor,
    endpoints_top2: Dict[str, Any],
    x_expected: torch.Tensor,
    x_actual: torch.Tensor,
    y: torch.Tensor,
) -> None:
    """Log candidate, best two endpoints from LineBO, and resultant expected, actual, y to terminal."""
    print("\n" + "=" * 60)
    print("[ITERATION LOG]")
    print("  candidate (x_tell):", candidate.cpu().numpy().tolist())
    if endpoints_top2:
        print("  best two endpoints (LineBO):")
        print("    line_0 left :", endpoints_top2.get("line_0_left", np.array([])).tolist())
        print("    line_0 right:", endpoints_top2.get("line_0_right", np.array([])).tolist())
        print("    line_1 left :", endpoints_top2.get("line_1_left", np.array([])).tolist())
        print("    line_1 right:", endpoints_top2.get("line_1_right", np.array([])).tolist())
    print("  expected (LineBO x_requested):")
    x_exp = x_expected.cpu().numpy()
    for i in range(min(3, len(x_exp))):
        print("   ", x_exp[i].tolist())
    if len(x_exp) > 3:
        print("    ... (%d points)" % len(x_exp))
    print("  actual (x_actual):")
    x_act = x_actual.cpu().numpy()
    for i in range(min(3, len(x_act))):
        print("   ", x_act[i].tolist())
    if len(x_act) > 3:
        print("    ... (%d points)" % len(x_act))
    y_flat = y.cpu().numpy().ravel()
    print("  y (objective values):", y_flat.tolist() if len(y_flat) <= 12 else y_flat[:6].tolist() + ["..."] + y_flat[-6:].tolist())
    print("=" * 60 + "\n")


def normalize_last_axis(arr: np.ndarray) -> np.ndarray:
    """Normalize array along last axis to sum to 1, handling edge cases."""
    a = np.asarray(arr, dtype=float)
    sums = a.sum(axis=-1, keepdims=True)
    sums = np.where(sums == 0, 1.0, sums)
    result = a / sums
    mask = ~np.isfinite(result).all(axis=-1, keepdims=True)
    if np.any(mask):
        d = a.shape[-1]
        result = np.where(mask, 1.0 / d, result)
    return result


def get_y_measurements(
    x,
    db: str = "./sql/objective.db",
    verbose: bool = False,
    ready_for_objectives: bool = False,
):
    """
    Read objective values (and compositions → x_meas) from the objective DB.

    Handshake (when ready_for_objectives=True):
    - Receiver sets handshake.new_objective_available = 1 when it writes a new objective row.
    - We wait until flag == 1, then read the objective table, then set flag = 0 (consumed).
    - This avoids reading stale data. On resume, reset_objective() clears the table and
      sets flag = 0 so the first read waits for fresh data from the apparatus.
    """
    import os as _os
    import sqlite3
    import time as _time

    consecutive_errors = 0
    max_consecutive_errors = 10

    if ready_for_objectives:
        while True:
            try:
                conn = sqlite3.connect(db, timeout=10.0)
                cur = conn.cursor()
                cur.execute(
                    """CREATE TABLE IF NOT EXISTS handshake (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    new_objective_available INTEGER DEFAULT 0
                )"""
                )
                cur.execute("INSERT OR IGNORE INTO handshake (id, new_objective_available) VALUES (1, 0)")
                cur.execute("SELECT new_objective_available FROM handshake WHERE id = 1")
                flag = cur.fetchone()
                conn.close()
                if flag and flag[0] == 1:
                    break
                _time.sleep(1)
            except Exception:
                _time.sleep(1)
                continue

    while True:
        try:
            if not _os.path.exists(db):
                _time.sleep(1)
                continue
            from scripts.communication import _objective_db_lock, _objective_writing

            if _objective_writing:
                _time.sleep(0.1)
                continue
            with _objective_db_lock:
                conn = sqlite3.connect(db, timeout=30.0)
                cur = conn.cursor()
                cur.execute("SELECT * FROM objective")
                all_rows = cur.fetchall()
                if not all_rows:
                    conn.close()
                    _time.sleep(1)
                    continue
                if len(all_rows) == 1 and len(all_rows[0]) > 1:
                    flat = list(all_rows[0])
                elif len(all_rows) > 1 and len(all_rows[0]) == 1:
                    flat = [r[0] for r in all_rows]
                else:
                    conn.close()
                    _time.sleep(1)
                    continue
                y_all = np.array(flat, dtype=float)
                valid_mask = ~np.isnan(y_all)
                valid_indices = np.where(valid_mask)[0]
                if len(valid_indices) == 0:
                    conn.close()
                    _time.sleep(1)
                    continue
                y = y_all[valid_indices].reshape(-1)
                cur.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name='compositions'",
                )
                if cur.fetchone():
                    cur.execute("SELECT * FROM compositions")
                    comp_rows = cur.fetchall()
                    X_meas_full = np.array(comp_rows, dtype=float) if comp_rows else None
                else:
                    X_meas_full = None
                if X_meas_full is not None and X_meas_full.shape[0] >= len(flat):
                    x_meas = X_meas_full[valid_indices][:, OPTIMIZING_DIMS]
                elif X_meas_full is not None and X_meas_full.shape[0] > 0:
                    x_meas = np.zeros((len(valid_indices), len(OPTIMIZING_DIMS)), dtype=float)
                    n_to_copy = min(X_meas_full.shape[0], len(valid_indices))
                    x_meas[:n_to_copy] = X_meas_full[:n_to_copy][:, OPTIMIZING_DIMS]
                else:
                    x_meas = np.zeros((len(valid_indices), len(OPTIMIZING_DIMS)), dtype=float)
                if verbose:
                    print(f"[get_y_measurements] ✅ NEW DATA RECEIVED: {len(y)} objective values")
                conn.close()
                break
        except Exception:
            consecutive_errors += 1
            if consecutive_errors >= max_consecutive_errors:
                _time.sleep(5.0)
                consecutive_errors = 0
            else:
                _time.sleep(1)
            continue

    if ready_for_objectives:
        try:
            conn = sqlite3.connect(db, timeout=10.0)
            cur = conn.cursor()
            cur.execute("UPDATE handshake SET new_objective_available = 0 WHERE id = 1")
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"[get_y_measurements] Error clearing handshake flag: {e}")
    return y, x_meas


def _pad_to_10d(arr):
    arr = np.atleast_2d(arr)
    out = np.zeros((arr.shape[0], 10), dtype=arr.dtype)
    out[:, OPTIMIZING_DIMS] = arr
    return out


def expected_from_actual(x_actual: torch.Tensor) -> torch.Tensor:
    """
    Compute expected (requested) points from actual points the same way LineBO does:
    points evenly spaced along the first principal direction of x_actual.
    """
    if x_actual.shape[0] > 1:
        x_centered = x_actual - x_actual.mean(dim=0, keepdim=True)
        U, S, V = torch.linalg.svd(x_centered, full_matrices=False)
        direction = V[0]  # (d,) first right singular vector
        projections = torch.matmul(x_centered, direction.unsqueeze(1)).squeeze(1)
        t_vals = torch.linspace(
            projections.min().item(),
            projections.max().item(),
            x_actual.shape[0],
            device=x_actual.device,
            dtype=x_actual.dtype,
        )
        x_requested = x_actual.mean(dim=0, keepdim=True) + t_vals.unsqueeze(1) * direction.unsqueeze(0)
    else:
        x_requested = x_actual.clone()
    return x_requested


def objective(
    endpoints: torch.Tensor,
    ready_for_objectives: bool = True,
    endpoints_log_ref: Dict[str, Any] | None = None,
    num_experiments: int = NUM_EXPERIMENTS,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Single objective: accepts (n, 2, d) torch tensor with n >= 2.
    Sends first two lines endpoints[0] and endpoints[1] (each 2,d = left, right) to communication,
    waits on response, returns (x_actual, y) for the first line as tensors.
    Logs the endpoints passed in to endpoints_log_ref when provided.
    """
    print(
        "[objective] called with params:\n"
        f"  endpoints.shape={getattr(endpoints, 'shape', None)}, dtype={getattr(endpoints, 'dtype', None)}, device={getattr(endpoints, 'device', None)}\n"
        f"  ready_for_objectives={ready_for_objectives}\n"
        f"  endpoints_log_ref keys={list(endpoints_log_ref.keys()) if endpoints_log_ref is not None else None}\n"
        f"  num_experiments={num_experiments}"
    )
    assert endpoints.dim() == 3 and endpoints.shape[0] >= 2 and endpoints.shape[1] == 2
    device = endpoints.device
    dtype = endpoints.dtype
    line0 = endpoints[0]  # (2, d) left, right
    line1 = endpoints[1]  # (2, d) left, right
    line_0_left = line0[0].cpu().numpy()
    line_0_right = line0[1].cpu().numpy()
    line_1_left = line1[0].cpu().numpy()
    line_1_right = line1[1].cpu().numpy()

    if endpoints_log_ref is not None:
        endpoints_log_ref["line_0_left"] = line_0_left
        endpoints_log_ref["line_0_right"] = line_0_right
        endpoints_log_ref["line_1_left"] = line_1_left
        endpoints_log_ref["line_1_right"] = line_1_right

    x_main = np.array([line_0_left + t * (line_0_right - line_0_left) for t in np.linspace(0, 1, num_experiments)])
    x_cache = np.array([line_1_left + t * (line_1_right - line_1_left) for t in np.linspace(0, 1, num_experiments)])
    left_norm = _pad_to_10d(normalize_last_axis(np.round(line_0_left, 3)))[0]
    right_norm = _pad_to_10d(normalize_last_axis(np.round(line_0_right, 3)))[0]
    x_main_norm = _pad_to_10d(normalize_last_axis(np.round(x_main, 3)))
    cache_left_norm = _pad_to_10d(normalize_last_axis(np.round(line_1_left, 3)))[0]
    cache_right_norm = _pad_to_10d(normalize_last_axis(np.round(line_1_right, 3)))[0]
    x_cache_norm = _pad_to_10d(normalize_last_axis(np.round(x_cache, 3)))

    communication.write_compositions(
        start=left_norm,
        end=right_norm,
        array=x_main_norm,
        start_cache=cache_left_norm,
        end_cache=cache_right_norm,
        array_cache=x_cache_norm,
        timestamp=time.time(),
    )
    # When waiting for apparatus: clear objective DB and handshake *after* sending compositions
    # so we only accept data that arrives in response to this request (avoids re-reading stale data on resume).
    if ready_for_objectives:
        communication.reset_objective()
    y_all, x_meas_all = get_y_measurements(
        np.vstack([x_main, x_cache]), verbose=True, ready_for_objectives=ready_for_objectives
    )
    x_meas_main = x_meas_all[:num_experiments].astype(np.float64)
    y_main = np.asarray(y_all[:num_experiments]).ravel().astype(np.float64)
    return (
        torch.tensor(x_meas_main, device=device, dtype=dtype),
        torch.tensor(y_main, device=device, dtype=dtype),
    )


def linebo_sampler_wrapper(
    dimensions: int,
    num_lines: int = 10,
    device: str = "cuda",
    dtype: torch.dtype = torch.float64,
    resume_plot_data: Tuple[List[float], List[float], List[np.ndarray]] | None = None,
    needle_plot_points: List[Dict[str, float]] | None = None,
):
    """
    Wrapper for LineBO.sampler: calls linebo.sampler, then logs expected, actual, y
    and makes them available to all logging (live plots + log_iteration).
    If resume_plot_data is provided (e.g. when resuming a job), prefill plot lists and redraw.
    needle_plot_points: mutable list of {sample_idx, y, distance} for needle stars on the plot.
    """
    if needle_plot_points is None:
        needle_plot_points = []
    endpoints_log_ref: Dict[str, Any] = {}
    linebo = LineBO(
        lambda ep: objective(ep, ready_for_objectives=True, endpoints_log_ref=endpoints_log_ref),
        dimensions,
        num_points_per_line=100,
        num_lines=num_lines,
        device=str(device),
    )
    fig_ref, all_sample_num, all_y, all_x_actual, _ = setup_live_plots()
    if resume_plot_data is not None:
        sample_nums, y_vals, x_actuals = resume_plot_data
        all_sample_num.extend(sample_nums)
        all_y.extend(y_vals)
        all_x_actual.extend(x_actuals)
        # Show once with loaded history (and needles if any)
        if _HAS_MPL and all_y:
            update_live_plots(
                fig_ref, all_sample_num, all_y, all_x_actual,
                np.zeros((0, dimensions)), np.array([]),
                needle_plot_points=needle_plot_points,
            )

    def wrapper(
        x_tell: torch.Tensor,
        bounds: torch.Tensor | None = None,
        acquisition_function=None,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        x_requested, x_actual, y = linebo.sampler(x_tell, bounds, acquisition_function)
        y_flat = y.reshape(-1)
        x_act_np = x_actual.cpu().numpy()
        y_np = y_flat.cpu().numpy()
        update_live_plots(
            fig_ref, all_sample_num, all_y, all_x_actual, x_act_np, y_np,
            needle_plot_points=needle_plot_points,
        )
        log_iteration(x_tell, endpoints_log_ref, x_requested, x_actual, y_flat)
        return x_requested, x_actual, y_flat

    return wrapper


def initial_lines_on_boundary(
    num_lines: int,
    bounds: torch.Tensor,
    device: torch.device,
    dtype: torch.dtype = torch.float64,
    max_retries: int = 10,
) -> np.ndarray:
    """
    Generate num_lines with endpoints on the simplex boundary.

    Sample num_lines interior points on the simplex and num_lines random
    zero-sum directions; for each point + direction, extrapolate to the
    simplex boundary (as in linebo.line_simplex_segment) to get (x_left, x_right).
    Returns ordered_endpoints of shape (num_lines, 2, d) where [i, 0] is left
    and [i, 1] is right endpoint on the boundary.
    """
    d = bounds.shape[1]
    low, high = bounds[0], bounds[1]
    # Interior points on simplex (sum=1, in [0,1])
    points = ZoMBIHop.random_simplex(num_lines, low, high, device=device)
    endpoints_list = []
    for i in range(num_lines):
        x0 = points[i]
        for _ in range(max_retries):
            direction = zero_sum_dirs(1, d, device=device, dtype=dtype).squeeze(0)
            seg = line_simplex_segment(x0, direction)
            if seg is not None:
                _t_min, _t_max, x_left, x_right = seg
                endpoints_list.append([x_left.cpu().numpy(), x_right.cpu().numpy()])
                break
        else:
            # Fallback: segment between two vertices (boundary edges)
            ei = torch.zeros(d, device=device, dtype=dtype)
            ej = torch.zeros(d, device=device, dtype=dtype)
            ei[i % d] = 1.0
            ej[(i + 1) % d] = 1.0
            endpoints_list.append([ei.cpu().numpy(), ej.cpu().numpy()])
    return np.array(endpoints_list)


def _load_bounds_from_run(run_dir: Path, device: torch.device, dtype: torch.dtype = torch.float64) -> torch.Tensor:
    """Load bounds tensor from a saved run directory (for resuming)."""
    current_state_file = run_dir / "current_state.txt"
    if not current_state_file.exists():
        raise FileNotFoundError(f"No current_state.txt in {run_dir}; cannot load bounds for resume.")
    label = current_state_file.read_text().strip()
    state_dir = run_dir / "states" / label
    if not state_dir.exists():
        raise FileNotFoundError(f"State directory {state_dir} not found; cannot load bounds.")
    tensors_path = state_dir / "tensors.pt"
    if not tensors_path.exists():
        raise FileNotFoundError(f"No tensors.pt in {state_dir}; cannot load bounds.")
    tensors = torch.load(tensors_path, map_location=device, weights_only=False)
    if "bounds" not in tensors:
        raise KeyError(f"tensors.pt in {state_dir} has no 'bounds' key.")
    return tensors["bounds"].to(device=device, dtype=dtype)


def _load_plot_data_from_run(run_dir: Path) -> Tuple[List[float], List[float], List[np.ndarray]] | None:
    """Load latest all_points (sample numbers, y values, x_actual) from resumed run for live plots. Returns None if missing."""
    current_state_file = run_dir / "current_state.txt"
    if not current_state_file.exists():
        return None
    label = current_state_file.read_text().strip()
    state_dir = run_dir / "states" / label
    csv_path = state_dir / "all_points.csv"
    if not csv_path.exists():
        return None
    all_sample_num: List[float] = []
    all_y: List[float] = []
    all_x_actual: List[np.ndarray] = []
    try:
        with open(csv_path, newline="") as f:
            reader = csv.DictReader(f)
            if not reader.fieldnames:
                return None
            x_cols = sorted([c for c in reader.fieldnames if c.startswith("x_actual_") and c[len("x_actual_"):].isdigit()], key=lambda c: int(c.split("_")[-1]))
            for row in reader:
                try:
                    y_val = float(row["y_value"])
                    x_vals = [float(row[c]) for c in x_cols]
                except (KeyError, ValueError):
                    continue
                all_sample_num.append(len(all_sample_num) + 1)
                all_y.append(y_val)
                all_x_actual.append(np.array(x_vals))
    except Exception:
        return None
    if not all_y:
        return None
    return (all_sample_num, all_y, all_x_actual)


def _load_needles_for_plot(
    run_dir: Path,
    all_x_actual: List[np.ndarray],
) -> List[Dict[str, float]]:
    """Load needle positions for live plot stars from resumed run (same state as all_points). Returns list of {sample_idx, y, distance}."""
    out: List[Dict[str, float]] = []
    if not all_x_actual:
        return out
    current_state_file = run_dir / "current_state.txt"
    if not current_state_file.exists():
        return out
    label = current_state_file.read_text().strip()
    state_dir = run_dir / "states" / label
    needles_path = state_dir / "needles_results.json"
    if not needles_path.exists():
        return out
    try:
        with open(needles_path) as f:
            needles_data = json.load(f)
    except Exception:
        return out
    X = np.array(all_x_actual)
    center = np.mean(X, axis=0)
    for rec in needles_data:
        try:
            pt = np.array(rec["point"], dtype=float)
            y_val = float(rec["value"])
        except (KeyError, TypeError, ValueError):
            continue
        # Closest point index in all_x_actual (1-based sample number)
        dists = np.linalg.norm(X - pt, axis=1)
        idx = int(np.argmin(dists))
        sample_idx = idx + 1
        distance = float(np.linalg.norm(pt - center))
        out.append({"sample_idx": sample_idx, "y": y_val, "distance": distance})
    return out


def run_zombi_main(resume_uuid: str | None = None):
    """Run DB-driven ZoMBI-Hop loop (new or resume)."""
    # Clear objective DB and handshake so the first read waits for fresh data (new run or resume).
    communication.reset_objective()

    dimensions = len(OPTIMIZING_DIMS)
    device = torch.device(DEVICE)
    dtype = torch.float64
    bounds = torch.zeros((2, dimensions), device=device, dtype=dtype)
    bounds[0] = 0.0
    bounds[1] = 1.0

    base_dir = Path("actual_runs")
    base_dir.mkdir(exist_ok=True)
    checkpoint_dir = base_dir / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)

    resume_plot_data: Tuple[List[float], List[float], List[np.ndarray]] | None = None
    needle_plot_points: List[Dict[str, float]] = []
    if resume_uuid is not None:
        run_dir = checkpoint_dir / f"run_{resume_uuid}"
        if run_dir.exists():
            resume_plot_data = _load_plot_data_from_run(run_dir)
            if resume_plot_data is not None:
                print(f"[Resume] Loaded {len(resume_plot_data[1])} points into live plot.")
                resume_needles = _load_needles_for_plot(run_dir, resume_plot_data[2])
                needle_plot_points.extend(resume_needles)
                if resume_needles:
                    print(f"[Resume] Loaded {len(resume_needles)} needle(s) for plot stars.")

    objective_wrapper = linebo_sampler_wrapper(
        dimensions=dimensions,
        num_lines=10,
        device=device,
        dtype=dtype,
        resume_plot_data=resume_plot_data,
        needle_plot_points=needle_plot_points,
    )

    if resume_uuid is None:
        print("=" * 80)
        print("STARTING NEW ZOMBIHOP TRIAL (DATABASE-DRIVEN)")
        print("=" * 80)
        print(f"Dimensions: {dimensions} (from OPTIMIZING_DIMS: {OPTIMIZING_DIMS})")
        print(f"Device: {device}")
        print("=" * 80 + "\n")

        print("Generating initial data via database...")
        ordered_endpoints = initial_lines_on_boundary(
            2 * NUM_INIT_DATA, bounds, device, dtype=torch.float64
        )
        n_total = len(ordered_endpoints)
        x_actual_list: List[torch.Tensor] = []
        x_expected_list: List[torch.Tensor] = []
        y_list: List[torch.Tensor] = []
        for i in range(NUM_INIT_DATA):
            idx0 = 2 * i
            idx1 = 2 * i + 1
            line0 = ordered_endpoints[idx0]  # (2, d) left, right
            line1 = ordered_endpoints[idx1]
            # Ensure main and cache are different (avoid same values in cache and real)
            if np.allclose(line0, line1, rtol=1e-6, atol=1e-8):
                idx1 = (2 * i + 2) % n_total
                if idx1 == idx0:
                    idx1 = (idx0 + 1) % n_total
                line1 = ordered_endpoints[idx1]
            ep = torch.tensor(
                np.stack([line0, line1], axis=0),
                device=device,
                dtype=torch.float64,
            )
            x_act, y_act = objective(ep, ready_for_objectives=False)
            x_exp = expected_from_actual(x_act)
            x_actual_list.append(x_act)
            x_expected_list.append(x_exp)
            y_list.append(y_act)
        X_init_actual = torch.cat(x_actual_list, dim=0)
        X_init_expected = torch.cat(x_expected_list, dim=0)
        Y_init = torch.cat(y_list, dim=0).reshape(-1, 1)

        optimizer = ZoMBIHop(
            objective=objective_wrapper,
            bounds=bounds,
            X_init_actual=X_init_actual,
            X_init_expected=X_init_expected,
            Y_init=Y_init,
            max_zooms=3,
            max_iterations=4,
            top_m_points=max(dimensions + 1, 4),
            n_restarts=50,
            raw=500,
            penalization_threshold=6.5e-5,
            penalty_num_directions=10 * dimensions,
            penalty_max_radius=0.33633,
            penalty_radius_step=None,
            convergence_pi_threshold=4.8e-5,
            input_noise_threshold_mult=2.0,
            output_noise_threshold_mult=0.5,
            n_consecutive_converged=5,
            max_gp_points=3000,
            acquisition_type="ucb",
            ucb_beta=0.1,
            device=str(device),
            dtype=dtype,
            run_uuid=None,
            checkpoint_dir=str(checkpoint_dir),
            max_checkpoints=50,
            verbose=True,
            needle_plot_points_ref=needle_plot_points,
        )

        print(f"✅ Starting new trial with UUID: {optimizer.run_uuid}")
    else:
        run_dir = checkpoint_dir / f"run_{resume_uuid}"
        if not run_dir.exists():
            raise FileNotFoundError(f"Checkpoint not found for UUID {resume_uuid} (expected {run_dir})")

        bounds_resumed = _load_bounds_from_run(run_dir, device, dtype=dtype)
        optimizer = ZoMBIHop(
            objective=objective_wrapper,
            bounds=bounds_resumed,
            X_init_actual=None,
            X_init_expected=None,
            Y_init=None,
            acquisition_type="ucb",
            ucb_beta=0.1,
            device=str(device),
            dtype=dtype,
            run_uuid=resume_uuid,
            checkpoint_dir=str(checkpoint_dir),
            max_checkpoints=50,
            verbose=True,
            needle_plot_points_ref=needle_plot_points,
        )
        print(
            f"✅ Resumed from activation={optimizer.current_activation}, "
            f"zoom={optimizer.current_zoom}, iteration={optimizer.current_iteration}\n"
        )

    print("=" * 80)
    print("STARTING OPTIMIZATION")
    print("=" * 80 + "\n")

    optimizer.run(max_activations=float("inf"), time_limit_hours=None)


if __name__ == "__main__":
    import sys
    resume_uuid = None
    if len(sys.argv) >= 2:
        a1 = sys.argv[1].strip().lower()
        if a1 in ("-h", "--help", "help"):
            print("Usage: python -m scripts.run_zombi_main [UUID]")
            print("  UUID     Resume this run (e.g. 6877). Omit for a new run.")
            sys.exit(0)
        resume_uuid = sys.argv[1]
    run_zombi_main(resume_uuid=resume_uuid)
